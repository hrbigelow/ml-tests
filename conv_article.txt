A Matrix Multiplication view of Convolutions

In this note, I show that 1D convolutions and transpose convolutions calculated
by PyTorch and TensorFlow can be replicated by simply multiplying the input by
a sparse square matrix.  Optionally, the input or output is processed by a
mask. 

[The Matrix]

The matrix is a sparse square matrix, with one copy of the filter elements
on each row, centered around the diagonal.  For a filter like this:

[filter.png]
[1D convolution filter]

This matrix is constructed from a filter, and choice of filter 'center'
element.

matrix = make_matrix(matrix_sz=10, filter=[1,2,3,2,1], filter_center=2)

the resulting matrix is:

[mat.png]
[square matrix with filter weights scanned across each row]


Multiplying this matrix by a vector of input elements is equivalent to
positioning the center of the filter on each input element and performing the
convolution, collecting the output.  After all, a single convolution is just a
dot product, and a matrix multiplication is a dot product of each row with the
input.

Choice of stride and padding strategy determine which of these convolutions
will be discarded, or 'down-sampled'.  Filtering output elements with a mask
does this:

conv_full = np.matmul(matrix, input)
conv_downsampled = do_mask(conv_full, mask) 

In contrast to a regular convolution, transpose convolution first 'up-samples'
the input with the mask and then does matrix multiplication:

input_upsampled = un_mask(input, mask)
conv = np.matmul(np.transpose(matrix, (1, 0)), input_upsampled)

[Upsampling, downsampling and the mask]

The operations above are defined here:

def do_mask(ary, mask):
    '''remove elements where False in mask'''
    return ary[mask]


def un_mask(ary, mask):
    '''insert zeros where False in mask'''
    it = iter(ary)
    return np.array([next(it) if m else 0 for m in mask])

[mask_unmask.jpg]

When chained together, shape is restored, although some values are replaced
with zeros.  In this diagram, hollow circles represent zeros, hollow squares
represent False values in the mask.

[Mask construction from Stride, Filter length and Padding]

The mask is constructed in three regions.  On the left is a region of False
elements of length left_filter - left_padding.   On the right is a similar
False region of length right_filter - right_padding.  Both of these regions
represent places where the filter overhangs the padded input. In the middle is
a set of single True values intercalated by stretches of False values of length
stride - 1.

[mask_regions.jpg]

The quantities left_filter and right_filter are defined in the following way.
By convention, we take the center of a filter to be the central element for
odd-length filters, and the right-of-center element for even-length filters.
The center element is what's placed on the diagonal of the matrix.

[filter_flanks.jpg]

[Dilation only affects the matrix]

The default choice of dilation = 1 means the filter is left untouched.
Dilation > 1 means the filter is augmented with (dilation - 1) zero elements
between each input element.  This affects only the matrix.  The mask is
affected only in the sense that the dilated filter is larger, so more padding
must be applied to make the edge positions considered valid.

[dilation2_matrix.jpg]
[Example of a matrix of dilation 2]

Other than this, the matrix/mask approach is identical for convolutions and
transpose convolutions.


[(Almost) Complete example]

So, we can take input, filter, padding strategy, and stride to construct a
matrix and a mask.  Then, use the matrix and mask to perform both a convolution
and a transpose convolution.  Finally, show that this matrix/mask approach produces
identical results as PyTorch and TensorFlow:

mask = make_mask(len(input), len(filter), stride, padding)
matrix = make_matrix(len(input), filter) 
mm_conv = do_mask(np.matmul(matrix, input), mask)
mm_convt = np.matmul(np.transpose(matrix, (1, 0)), un_mask(mm_conv, mask))
assert len(input) == len(mm_convt)

# Compare matrix/mask results with PyTorch
th_conv = un_wrap(F.conv1d(do_wrap(input), wrap(filter), None, stride, padding, dilation, 1))
th_convt = un_wrap(F.conv_transpose1d(wrap(th_conv), wrap(filter), None,
    stride, padding, output_padding, groups=1, dilation))

assert all(mm_conv == th_conv)
assert all(mm_convt == th_convt)

# Compare matrix/mask results with TensorFlow 
tf_conv = un_wrap(tf.nn.convolution(wrap(input), wrap(filter), padding, [stride], [dilation]))
output_shape = tf.constant([1, len(input), 1]) 
dilated_filter = dilate_array(filter, dilation)
tf_convt = un_wrap(tf.contrib.nn.conv1d_transpose(wrap(tf_conv), wrap(dilated_filter),
    output_shape, stride, padding))

assert all(mm_conv == tf_conv)
assert all(mm_convt == tf_convt)

This experiment works for a large range of input sizes, filter sizes, stride,
padding, and dilation.  A small wrinkle is that TensorFlow's conv1d_transpose
doesn't support dilations > 1 if stride > 1.  But, we can simulate that by
pre-constructing a dilated filter.  Another wrinkle is that TensorFlow supports
padding types 'VALID' and 'SAME', while PyTorch accepts a single integer
padding.  For even-length filters, this means that it is impossible to produce
a logically 'SAME' padding for every possible input element.

[Generalizing to higher spatial dimensions]

I've illustrated that PyTorch and TensorFlow's convolutions and transpose
convolutions in one spatial dimension are mathematically equivalent to the
matrix/mask approach.  It turns out that this is true in two or more spatial
dimensions by first "unwrapping" both the filter and input into one dimension.

[Input and Output channels]

In this discussion I said that a single convolution is a dot product between
filter elements and input elements.  But, a dot product is just the sum of
pairwise multiplied elements.  One pair could be a scalar filter element and
scalar input element.  Or, it could be a matrix filter element and vector input
element.  Then the multiplication is just matrix-vector multiplication, and the
final addition is just vector-vector addition.  Either way, nothing about the
higher level organization of this is changed.


[Some perspective]

One nice feature of this view of convolution or transposed convolution is that
it defines a correspondence between input and output elements.  Both the choice
of stride and padding affect the number of output elements produced.  But, this
correspondence makes clear which elements are omitted (or up-sampled) due to
stride or edge effects. 

For instance, here are masks for two convolutions (T = True, _ = False):

[_, _, T, _, T, _, T, _, T, _, _, _] # stride 2, padding VALID

[_, T, _, _, T, _, _, T, _, _, T, _] # stride 3, padding SAME

Both of these cases give four output elements.  But we can see at a glance the
difference in where the information derives. 

