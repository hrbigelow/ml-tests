A Matrix Multiplication view of Convolutions

In this note, I show that 1D convolutions and transpose convolutions calculated
by PyTorch and TensorFlow can be replicated by simply multiplying the input by
a sparse square matrix.  Optionally, the input or output is processed by a
mask. 

[The Matrix]

The matrix is a sparse square matrix, with one copy of the filter elements
on each row, centered around the diagonal.  For a filter like this:

[filter.png]
[1D convolution filter]

This matrix is constructed from a filter, and choice of filter 'center'
element.

matrix = make_matrix(matrix_sz=10, filter=[1,2,3,2,1], filter_center=2)

the resulting matrix is:

[mat.png]
[square matrix with filter weights scanned across each row]


Multiplying this matrix by a vector of input elements is equivalent to
positioning the center of the filter on each input element and performing the
convolution, collecting the output.  After all, a single convolution is just a
dot product, and a matrix multiplication is a dot product of each row with the
input.

Choice of stride and padding strategy determine which of these convolutions
will be discarded, or 'down-sampled'.  Filtering output elements with a mask
does this:

conv_full = np.matmul(matrix, input)
conv_downsampled = do_mask(conv_full, mask) 

In contrast to a regular convolution, transpose convolution first 'up-samples'
the input with the mask and then does matrix multiplication:

input_upsampled = un_mask(input, mask)
conv = np.matmul(np.transpose(matrix, (1, 0)), input_upsampled)

[Upsampling, downsampling and the mask]

The operations above are defined here:

def do_mask(ary, mask):
    '''remove elements where False in mask'''
    return ary[mask]


def un_mask(ary, mask):
    '''insert zeros where False in mask'''
    it = iter(ary)
    return np.array([next(it) if m else 0 for m in mask])

[mask_unmask.jpg]

When chained together, shape is restored, although some values are replaced
with zeros.  In this diagram, hollow circles represent zeros, hollow squares
represent False values in the mask.

[Mask construction from Stride, Filter length and Padding]

The mask is constructed in three regions.  On the left is a region of False
elements of length left_filter - left_padding.   On the right is a similar
False region of length right_filter - right_padding.  Both of these regions
represent places where the filter overhangs the padded input. In the middle is
a set of single True values intercalated by stretches of False values of length
stride - 1.

[mask_regions.jpg]

The quantities left_filter and right_filter are defined in the following way.
By convention, we take the center of a filter to be the central element for
odd-length filters, and the right-of-center element for even-length filters.
The center element is what's placed on the diagonal of the matrix.

[filter_flanks.jpg]

[Complete example]

So, overall, we have taken a filter, padding strategy, and stride to construct
a matrix and a mask.  We can now use the same matrix and mask to perform both a
convolution and a transpose convolution with this approach as follows:

mask = make_mask(input_sz, filter_sz, stride, padding)
matrix = make_matrix(len(mask), filter) 
mm_conv = do_mask(np.matmul(matrix, input), mask)
mm_convt = np.matmul(np.transpose(matrix, (1, 0)), un_mask(mm_conv, mask))
assert len(input) == len(mm_convt)

As an aside, astute readers will notice that I've used input_sz to make the
mask, but len(mask) to construct the matrix.  It turns out that the mask is
sometimes slightly shorter than input_sz.  This is because I've designed these
functions to create a mask that can replicate the results of PyTorch and
TensorFlow for the settings of stride and padding.  
 

[One-to-one correspondence between input and output]

One nice feature of this view of convolution or transposed convolution is that
it defines a correspondence between input and output elements.  Both the choice
of stride and padding affect the number of output elements produced.  But, this
correspondence makes clear which elements are omitted (or up-sampled) due to
stride or edge effects. 

For instance, here are masks for two convolutions (T = True, _ = False):

[_, _, T, _, T, _, T, _, T, _, _, _] # stride 2, padding VALID

[_, T, _, _, T, _, _, T, _, _, T, _] # stride 3, padding SAME

Both of these cases give four output elements.  But we can see at a glance the
difference in where the information derives. 

Choice of stride and padding affect the mask but not the matrix.  Also, a
convolution and transposed convolution using the same stride and padding
settings also use the same mask. 

On the other hand, dilation of the filter has no effect on the mask.  But it
affects the matrix, since the matrix is entirely determined by the filter.
To illustrate, suppose you have this filter:

[dilated_filter.png]
[filter of size 5 with dilation=2]

The matrix would be:

[dilated_matrix.png]
[matrix with a filter size 5, dilation=2]

Everything else about the calculation would be the same, however.

[Particulars of TensorFlow and PyTorch convolutions]

Stride and padding don't fully determine a mask.  For instance, here are all
possible masks for stride=3, padding=SAME, filter length 5, and input length
13:

[T, _, _, T, _, _, T, _, _, T, _, _, T] # phase 0, output size 5
[_, T, _, _, T, _, _, T, _, _, T, _, _] # phase 1, output size 4
[_, _, T, _, _, T, _, _, T, _, _, T, _] # phase 2, output size 4

TensorFlow chooses phase 0 to maximize number of output elements.  On the other
hand, consider these masks with filter size 5, padding=SAME:

[T, _, _, _, T, _, _, _, T, _, _, _, T, _, _] # phase 0, output size 4
[_, T, _, _, _, T, _, _, _, T, _, _, _, T, _] # phase 1, output size 4
[_, _, T, _, _, _, T, _, _, _, T, _, _, _, T] # phase 2, output size 4
[_, _, _, T, _, _, _, T, _, _, _, T, _, _, _] # phase 3, output size 3


TensorFlow has a choice of phases 0, 1, or 2, which all produce the maximum
possible output size 4.  It chooses phase 1 because this avoids the
convolutions at the very ends which use 2 padding elements. 

PyTorch treats padding differently.  Instead of 'VALID' and 'SAME', it requires
an integer padding argument to be applied to both left and right.  In
particular, for filter of size 4, TensorFlow will apply 1 padding element on
the left, and 2 on the right of the input.  This allows the filter to cover all
ten positions with its 2nd element.  In the case of PyTorch, choosing padding=1
produces output size 9.  Choosing padding=2 yields an output size 11.




