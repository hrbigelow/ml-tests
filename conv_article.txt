A Matrix Multiplication view of Convolutions

In this note, I show that 1D convolutions and transpose convolutions calculated
by PyTorch and TensorFlow can be replicated by simply multiplying the input by
a sparse square matrix.  Optionally, the input or output is processed by a
mask. 

[The Matrix]

The matrix is a sparse square matrix, with one copy of the filter elements
on each row, centered around the diagonal.  For a filter like this:

[filter.png]
[1D convolution filter]

the matrix would be:

[mat_s1_ph0_inv0_lp0_rp0_d0.png]
[square matrix with filter weights scanned across each row]

This matrix is constructed from a filter, and designating which filter element
to place on the diagonal of the matrix:

filter = [1,2,3,2,1]
c = 2 # index of the center filter element

fl, fc, fr = filter[0:c], filter[c:c+1], filter[c+1:] # [1,2], [3], [2,1]
z = [0] * (matrix_sz - len(filter) + 1)
cells = (fc + fr + z + fl) * (matrix_sz - 1) + fc
mat = np.array(cells).reshape(matrix_sz, matrix_sz)


Multiplying this matrix by a vector of input elements is equivalent to
positioning the center of the filter on each input element and performing the
convolution, collecting the output.  After all, a convolution is just a dot
product, and a matrix multiplication is a dot product of each row with the
input.

Choice of stride and padding strategy determine which of these convolutions
will be retained.  Filtering output elements with a mask can do this:

conv_pre = np.matmul(matrix, input)
conv = do_mask(conv_pre, mask) # defined below

The transpose convolution first 'up-samples' the input with the mask and then
does matrix multiplication.

input_processed = un_mask(input, mask) # defined below
conv = np.matmul(matrix_transpose, input_processed)

Because of these masking operations, a convolution produces output that is the
same size or smaller than the input.  It is also known as 'down-sampling'.
Transpose convolutions produce output that is the same size or larger than the
input, also called 'up-sampling'.


[Upsampling, downsampling and the mask]

The operations above are defined here:

def do_mask(ary, mask):
    '''remove elements where False in mask'''
    return ary[np.where(mask)]

def un_mask(ary, mask):
    '''insert zeros where False in mask'''
    it = iter(ary)
    return np.array([next(it) if m else 0 for m in mask])

a = np.array([1,2,3])
m = np.array([True, False, False, True, True])
u = un_mask(a, m)  # [1, 0, 0, 2, 3]
b = do_mask(u, m)  # [1, 2, 3]
assert all(b == a)

For a convolution, the amount of down-sampling equals the number of False
values in the mask.  For a transpose convolution, the amount of up-sampling
also equals the number of False values.

[Complete example]

Using the same mask for a convolution and then a transpose convolution will
recover the original input size.  It won't recover the same input *values*,
because there is loss of information during down-sampling.  But, the two
operations are meant to be inverses of each other with respect to size changes.
Here is a complete example of a convolution followed by a transpose
convolution:



[One-to-one correspondence between input and output]

One nice feature of this view of convolution or transposed convolution is that
it defines a correspondence between input and output elements.  Both the choice
of stride and padding affect the number of output elements produced.  But, this
correspondence makes clear which elements are omitted (or up-sampled) due to
stride or edge effects. 

For instance, here are masks corresponding to two convolutions (T = True, _ =
False):

[_, _, T, _, T, _, T, _, T, _, _, _] # stride 2, padding VALID

[_, T, _, _, T, _, _, T, _, _, T, _] # stride 3, padding SAME

Both of these cases give four output elements.  But we can see at a glance the
difference in where the information derives. 

Choice of stride and padding affect the mask but not the matrix.  Also, a
convolution and transposed convolution using the same stride and padding
settings also use the same mask. 

On the other hand, dilation of the filter has no effect on the mask.  But it
affects the matrix, since the matrix is entirely determined by the filter.
To illustrate, suppose you have this filter:

[dilated_filter.png]
[filter of size 5 with dilation=2]

The matrix would be:

[dilated_matrix.png]
[matrix with a filter size 5, dilation=2]

Everything else about the calculation would be the same, however.

[Particulars of TensorFlow and PyTorch convolutions]

Stride and padding don't fully determine a mask.  For instance, here are all
possible masks for stride=3, padding=SAME, filter length 5, and input length
13:

[T, _, _, T, _, _, T, _, _, T, _, _, T] # phase 0, output size 5
[_, T, _, _, T, _, _, T, _, _, T, _, _] # phase 1, output size 4
[_, _, T, _, _, T, _, _, T, _, _, T, _] # phase 2, output size 4

TensorFlow chooses phase 0 to maximize number of output elements.  On the other
hand, consider these masks with filter size 5, padding=SAME:

[T, _, _, _, T, _, _, _, T, _, _, _, T, _, _] # phase 0, output size 4
[_, T, _, _, _, T, _, _, _, T, _, _, _, T, _] # phase 1, output size 4
[_, _, T, _, _, _, T, _, _, _, T, _, _, _, T] # phase 2, output size 4
[_, _, _, T, _, _, _, T, _, _, _, T, _, _, _] # phase 3, output size 3


TensorFlow has a choice of phases 0, 1, or 2, which all produce the maximum
possible output size 4.  It chooses phase 1 because this avoids the
convolutions at the very ends which use 2 padding elements. 

PyTorch treats padding differently.  Instead of 'VALID' and 'SAME', it requires
an integer padding argument to be applied to both left and right.  In
particular, for filter of size 4, TensorFlow will apply 1 padding element on
the left, and 2 on the right of the input.  This allows the filter to cover all
ten positions with its 2nd element.  In the case of PyTorch, choosing padding=1
produces output size 9.  Choosing padding=2 yields an output size 11.




